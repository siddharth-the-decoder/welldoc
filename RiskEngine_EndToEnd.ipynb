{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RiskEngine End-to-End (30–180d history → 90d risk)\n",
        "This notebook builds a risk prediction pipeline over your four CSVs.\n",
        "\n",
        "**Inputs expected:**\n",
        "- `Patient_info.csv`\n",
        "- `Biochemical_parameters.csv`\n",
        "- `Diagnostics.csv`\n",
        "- `Glucose_measurements.csv` (large; we'll aggregate to daily metrics)\n",
        "\n",
        "**Outputs:**\n",
        "- `outputs/feature_dataset.parquet` – modeling dataset with rolling-window features\n",
        "- `outputs/model_calibrated.joblib` – trained + calibrated model\n",
        "- `outputs/metrics.json` – AUROC, AUPRC, Brier, confusion matrix\n",
        "- `outputs/plots/` – ROC, PR, Calibration plots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using XGBoost: True | SHAP available: True\n"
          ]
        }
      ],
      "source": [
        "# --- Setup ---\n",
        "import os, json, math, gc, sys, warnings\n",
        "from pathlib import Path\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, confusion_matrix\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Prefer xgboost; fallback to HistGradientBoosting if not available\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    HAS_XGB = True\n",
        "except Exception:\n",
        "    from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "    HAS_XGB = False\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    HAS_SHAP = True\n",
        "except Exception:\n",
        "    HAS_SHAP = False\n",
        "\n",
        "BASE = Path(\".\").resolve()\n",
        "OUT = BASE / \"outputs\"\n",
        "PLOTS = OUT / \"plots\"\n",
        "for p in [OUT, PLOTS]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Using XGBoost:\", HAS_XGB, \"| SHAP available:\", HAS_SHAP)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Config: set your file paths here ---\n",
        "PATH_PATIENT_INFO = \"Patient_info.csv\"\n",
        "PATH_LABS = \"Biochemical_parameters.csv\"\n",
        "PATH_DIAG = \"Diagnostics.csv\"\n",
        "PATH_CGM = \"Glucose_measurements.csv\"   # large file\n",
        "\n",
        "# CGM cadence assumption (15-min ~ 96/day)\n",
        "EXPECTED_READS_PER_DAY = 96\n",
        "\n",
        "# Rolling windows (in days)\n",
        "WINDOWS = [30, 90, 180]\n",
        "\n",
        "# Label thresholds\n",
        "FUTURE_90D_HYPER_MEAN_THRESHOLD = 0.35   # avg future 90d fraction > 180 mg/dL\n",
        "A1C_RISE_THRESHOLD = 0.5                 # A1c increase >= 0.5 absolute within 90d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Patients: 736 | Labs coverage: 723 | Diagnostics coverage: 511\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(  Patient_ID Reception_date                              Name  Value\n",
              " 0  LIB193265     2018-05-09                         Potassium    4.5\n",
              " 1  LIB193265     2018-05-09                   HDL cholesterol   64.0\n",
              " 2  LIB193265     2018-05-09  Gamma-glutamyl Transferase (GGT)   11.0,\n",
              "   Patient_ID    Code                                        Description\n",
              " 0  LIB193263   272.4               Other and unspecified hyperlipidemia\n",
              " 1  LIB193264   354.0                             Carpal tunnel syndrome\n",
              " 2  LIB193264  574.00  Calculus of gallbladder with acute cholecystit...,\n",
              "   Patient_ID Sex  Birth_year Initial_measurement_date Final_measurement_date  \\\n",
              " 0  LIB193263   M        1965               2020-09-06             2022-03-19   \n",
              " 1  LIB193264   F        1975               2020-10-06             2022-03-19   \n",
              " 2  LIB193265   F        1980                      NaT             2022-03-19   \n",
              " \n",
              "    Number_of_days_with_measures  Number_of_measurements  \\\n",
              " 0                           648                   60097   \n",
              " 1                           326                   26786   \n",
              " 2                           581                   46575   \n",
              " \n",
              "   Initial_biochemical_parameters_date Final_biochemical_parameters_date  \\\n",
              " 0                                 NaT                               NaT   \n",
              " 1                                 NaT                               NaT   \n",
              " 2                          2018-05-09                        2021-01-10   \n",
              " \n",
              "    Number_of_biochemical_parameters  Number_of_diagnostics  \n",
              " 0                               NaN                    1.0  \n",
              " 1                               NaN                    3.0  \n",
              " 2                             120.0                    NaN  )"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# --- Load the three small tables ---\n",
        "pi = pd.read_csv(PATH_PATIENT_INFO)\n",
        "bp = pd.read_csv(PATH_LABS)\n",
        "dg = pd.read_csv(PATH_DIAG)\n",
        "\n",
        "# Parse dates in patient_info\n",
        "for c in [\"Initial_measurement_date\",\"Final_measurement_date\",\n",
        "          \"Initial_biochemical_parameters_date\",\"Final_biochemical_parameters_date\"]:\n",
        "    if c in pi.columns:\n",
        "        pi[c] = pd.to_datetime(pi[c], dayfirst=True, errors=\"coerce\")\n",
        "\n",
        "# Parse labs\n",
        "if \"Reception_date\" in bp.columns:\n",
        "    bp[\"Reception_date\"] = pd.to_datetime(bp[\"Reception_date\"], dayfirst=True, errors=\"coerce\")\n",
        "\n",
        "print(\"Patients:\", pi['Patient_ID'].nunique(), \n",
        "      \"| Labs coverage:\", bp['Patient_ID'].nunique(), \n",
        "      \"| Diagnostics coverage:\", dg['Patient_ID'].nunique())\n",
        "bp.head(3), dg.head(3), pi.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "If ready, run aggregate_cgm_daily(PATH_CGM) to produce daily aggregates → outputs/daily_cgm.parquet\n"
          ]
        }
      ],
      "source": [
        "    # --- Aggregate CGM to daily features (chunked) ---\n",
        "    # Produces a per-patient-per-day summary to keep memory in check.\n",
        "\n",
        "    def aggregate_cgm_daily(path_cgm, expected_reads_per_day=96, chunksize=2_000_000):\n",
        "        cols = [\"Patient_ID\",\"Measurement_date\",\"Measurement_time\",\"Measurement\"]\n",
        "        usecols = None  # if your file has extra cols, we can restrict: usecols=cols\n",
        "        dtypes = {\"Patient_ID\":\"category\", \"Measurement\":\"float32\"}\n",
        "        out_frames = []\n",
        "\n",
        "        # We'll accumulate in a dict to avoid too many frames\n",
        "        agg = {}\n",
        "\n",
        "        for chunk in pd.read_csv(path_cgm, usecols=usecols, dtype=dtypes,\n",
        "                                chunksize=chunksize, low_memory=True):\n",
        "            # combine date + time into timestamp then extract date\n",
        "            # note: parsing as datetime is expensive; we only need date-level\n",
        "            # We'll keep Measurement_date (dayfirst) and ignore time for daily group\n",
        "            chunk[\"Measurement_date\"] = pd.to_datetime(chunk[\"Measurement_date\"], dayfirst=True, errors=\"coerce\").dt.date\n",
        "            chunk = chunk.dropna(subset=[\"Measurement_date\",\"Measurement\",\"Patient_ID\"])\n",
        "            # basic flags\n",
        "            chunk[\"gt_180\"] = (chunk[\"Measurement\"] > 180).astype(\"int8\")\n",
        "            chunk[\"lt_70\"]  = (chunk[\"Measurement\"] < 70).astype(\"int8\")\n",
        "            # groupby patient/day\n",
        "            grp = chunk.groupby([\"Patient_ID\",\"Measurement_date\"]).agg(\n",
        "                n_reads=(\"Measurement\",\"size\"),\n",
        "                mean_glu=(\"Measurement\",\"mean\"),\n",
        "                std_glu=(\"Measurement\",\"std\"),\n",
        "                max_glu=(\"Measurement\",\"max\"),\n",
        "                min_glu=(\"Measurement\",\"min\"),\n",
        "                gt_180_sum=(\"gt_180\",\"sum\"),\n",
        "                lt_70_sum=(\"lt_70\",\"sum\"),\n",
        "            ).reset_index()\n",
        "\n",
        "            for _, r in grp.iterrows():\n",
        "                key = (r[\"Patient_ID\"], r[\"Measurement_date\"])\n",
        "                if key not in agg:\n",
        "                    agg[key] = {\n",
        "                        \"Patient_ID\": r[\"Patient_ID\"],\n",
        "                        \"date\": pd.to_datetime(str(r[\"Measurement_date\"])),\n",
        "                        \"n_reads\": int(r[\"n_reads\"]),\n",
        "                        \"sum_glu\": float(r[\"mean_glu\"] * r[\"n_reads\"]),\n",
        "                        \"sum_sq_glu\": float((r[\"std_glu\"]**2 + r[\"mean_glu\"]**2) * r[\"n_reads\"]) if not pd.isna(r[\"std_glu\"]) else float(r[\"mean_glu\"]**2 * r[\"n_reads\"]),\n",
        "                        \"max_glu\": float(r[\"max_glu\"]),\n",
        "                        \"min_glu\": float(r[\"min_glu\"]),\n",
        "                        \"gt_180\": int(r[\"gt_180_sum\"]),\n",
        "                        \"lt_70\": int(r[\"lt_70_sum\"]),\n",
        "                    }\n",
        "                else:\n",
        "                    a = agg[key]\n",
        "                    a[\"n_reads\"] += int(r[\"n_reads\"])\n",
        "                    a[\"sum_glu\"] += float(r[\"mean_glu\"] * r[\"n_reads\"])\n",
        "                    a[\"sum_sq_glu\"] += float((r[\"std_glu\"]**2 + r[\"mean_glu\"]**2) * r[\"n_reads\"]) if not pd.isna(r[\"std_glu\"]) else float(r[\"mean_glu\"]**2 * r[\"n_reads\"])\n",
        "                    a[\"max_glu\"] = max(a[\"max_glu\"], float(r[\"max_glu\"]))\n",
        "                    a[\"min_glu\"] = min(a[\"min_glu\"], float(r[\"min_glu\"]))\n",
        "                    a[\"gt_180\"] += int(r[\"gt_180_sum\"])\n",
        "                    a[\"lt_70\"] += int(r[\"lt_70_sum\"])\n",
        "\n",
        "            del chunk, grp\n",
        "            gc.collect()\n",
        "\n",
        "        # materialize\n",
        "        rows = []\n",
        "        for (pid, day), a in agg.items():\n",
        "            mean = a[\"sum_glu\"] / max(1, a[\"n_reads\"])\n",
        "            # variance = E[x^2] - (E[x])^2\n",
        "            var = max(0.0, a[\"sum_sq_glu\"]/max(1, a[\"n_reads\"]) - mean**2)\n",
        "            std = math.sqrt(var)\n",
        "            rows.append({\n",
        "                \"Patient_ID\": pid,\n",
        "                \"date\": a[\"date\"],\n",
        "                \"n_reads\": a[\"n_reads\"],\n",
        "                \"coverage\": min(1.0, a[\"n_reads\"] / expected_reads_per_day),\n",
        "                \"mean_glu\": mean,\n",
        "                \"std_glu\": std,\n",
        "                \"max_glu\": a[\"max_glu\"],\n",
        "                \"min_glu\": a[\"min_glu\"],\n",
        "                \"frac_gt_180\": a[\"gt_180\"] / max(1, a[\"n_reads\"]),\n",
        "                \"frac_lt_70\": a[\"lt_70\"] / max(1, a[\"n_reads\"]),\n",
        "                \"tir_70_180\": max(0.0, 1.0 - (a[\"gt_180\"] + a[\"lt_70\"]) / max(1, a[\"n_reads\"])),\n",
        "            })\n",
        "        daily = pd.DataFrame(rows).sort_values([\"Patient_ID\",\"date\"])\n",
        "        return daily\n",
        "\n",
        "    # NOTE: Running this on the full CGM file can take time. Uncomment when ready.\n",
        "    # daily_cgm = aggregate_cgm_daily(PATH_CGM, EXPECTED_READS_PER_DAY)\n",
        "    # daily_cgm.to_parquet(OUT / \"daily_cgm.parquet\", index=False)\n",
        "    print(\"If ready, run aggregate_cgm_daily(PATH_CGM) to produce daily aggregates → outputs/daily_cgm.parquet\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Daily CGM parquet not found. Generate it in the previous cell when you have the big file locally.\n"
          ]
        }
      ],
      "source": [
        "# --- Quick load of pre-aggregated daily CGM if you've already generated it ---\n",
        "daily_path = OUT / \"daily_cgm.parquet\"\n",
        "if daily_path.exists():\n",
        "    daily_cgm = pd.read_parquet(daily_path)\n",
        "    print(\"Loaded daily CGM:\", daily_cgm.shape)\n",
        "    display(daily_cgm.head())\n",
        "else:\n",
        "    print(\"Daily CGM parquet not found. Generate it in the previous cell when you have the big file locally.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A1c rows: (1963, 3)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Patient_ID</th>\n",
              "      <th>date</th>\n",
              "      <th>a1c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>LIB193265</td>\n",
              "      <td>2018-05-09</td>\n",
              "      <td>8.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>LIB193265</td>\n",
              "      <td>2019-12-03</td>\n",
              "      <td>7.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>LIB193265</td>\n",
              "      <td>2020-02-06</td>\n",
              "      <td>7.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>LIB193266</td>\n",
              "      <td>2021-05-04</td>\n",
              "      <td>7.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>LIB193266</td>\n",
              "      <td>2021-05-11</td>\n",
              "      <td>7.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Patient_ID       date  a1c\n",
              "11   LIB193265 2018-05-09  8.6\n",
              "33   LIB193265 2019-12-03  7.5\n",
              "60   LIB193265 2020-02-06  7.3\n",
              "165  LIB193266 2021-05-04  7.5\n",
              "169  LIB193266 2021-05-11  7.5"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# --- Prepare A1c time series per patient (for features + label proxy) ---\n",
        "# We filter labs to A1c and keep (Patient_ID, date, value)\n",
        "a1c = bp[bp[\"Name\"].str.contains(\"Glycated hemoglobin\", case=False, na=False)].copy()\n",
        "a1c = a1c.rename(columns={\"Reception_date\":\"date\",\"Value\":\"a1c\"})\n",
        "a1c = a1c[[\"Patient_ID\",\"date\",\"a1c\"]].dropna()\n",
        "a1c = a1c.sort_values([\"Patient_ID\",\"date\"])\n",
        "print(\"A1c rows:\", a1c.shape)\n",
        "display(a1c.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diagnostics one-hot shape: (511, 16)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Code</th>\n",
              "      <th>Patient_ID</th>\n",
              "      <th>diag_244.9</th>\n",
              "      <th>diag_250.4</th>\n",
              "      <th>diag_250.5</th>\n",
              "      <th>diag_250.51</th>\n",
              "      <th>diag_250.6</th>\n",
              "      <th>diag_272.0</th>\n",
              "      <th>diag_272.4</th>\n",
              "      <th>diag_278.00</th>\n",
              "      <th>diag_354.0</th>\n",
              "      <th>diag_362.01</th>\n",
              "      <th>diag_401.9</th>\n",
              "      <th>diag_477.0</th>\n",
              "      <th>diag_477.9</th>\n",
              "      <th>diag_493.9</th>\n",
              "      <th>diag_733.00</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LIB193263</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LIB193264</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LIB193266</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LIB193267</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LIB193269</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Code Patient_ID  diag_244.9  diag_250.4  diag_250.5  diag_250.51  diag_250.6  \\\n",
              "0     LIB193263           0           0           0            0           0   \n",
              "1     LIB193264           0           0           0            0           0   \n",
              "2     LIB193266           0           0           0            0           0   \n",
              "3     LIB193267           0           0           0            0           0   \n",
              "4     LIB193269           0           0           0            0           0   \n",
              "\n",
              "Code  diag_272.0  diag_272.4  diag_278.00  diag_354.0  diag_362.01  \\\n",
              "0              0           1            0           0            0   \n",
              "1              0           0            0           1            0   \n",
              "2              0           0            0           0            0   \n",
              "3              0           0            0           0            0   \n",
              "4              0           0            0           0            0   \n",
              "\n",
              "Code  diag_401.9  diag_477.0  diag_477.9  diag_493.9  diag_733.00  \n",
              "0              0           0           0           0            0  \n",
              "1              0           0           0           0            0  \n",
              "2              0           0           0           0            0  \n",
              "3              0           0           0           0            0  \n",
              "4              0           0           0           0            0  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# --- Diagnostics flags (static, per patient) ---\n",
        "# Create binary flags for top-15 most frequent codes to avoid high dimensionality.\n",
        "top_codes = dg[\"Code\"].value_counts().head(15).index.tolist()\n",
        "diag_flags = (dg.assign(val=1)\n",
        "                .pivot_table(index=\"Patient_ID\", columns=\"Code\", values=\"val\", aggfunc=\"max\", fill_value=0))\n",
        "# keep only top codes\n",
        "diag_flags = diag_flags[[c for c in diag_flags.columns if c in top_codes]]\n",
        "diag_flags = diag_flags.add_prefix(\"diag_\").reset_index()\n",
        "print(\"Diagnostics one-hot shape:\", diag_flags.shape)\n",
        "display(diag_flags.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "daily_cgm not loaded. Generate or load outputs/daily_cgm.parquet first.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Build rolling-window features and labels ---\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mdaily_cgm\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdaily_cgm not loaded. Generate or load outputs/daily_cgm.parquet first.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Helper: compute backward rolling (30/90/180) for select columns per patient\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_rolling_features\u001b[39m(df, cols, windows):\n",
            "\u001b[31mRuntimeError\u001b[39m: daily_cgm not loaded. Generate or load outputs/daily_cgm.parquet first."
          ]
        }
      ],
      "source": [
        "# --- Build rolling-window features and labels ---\n",
        "if 'daily_cgm' not in globals():\n",
        "    raise RuntimeError(\"daily_cgm not loaded. Generate or load outputs/daily_cgm.parquet first.\")\n",
        "\n",
        "# Helper: compute backward rolling (30/90/180) for select columns per patient\n",
        "def add_rolling_features(df, cols, windows):\n",
        "    df = df.sort_values([\"Patient_ID\",\"date\"]).copy()\n",
        "    for w in windows:\n",
        "        for c in cols:\n",
        "            df[f\"{c}_mean_{w}\"] = (df.groupby(\"Patient_ID\")[c]\n",
        "                                     .transform(lambda s: s.rolling(window=w, min_periods=7).mean()))\n",
        "            df[f\"{c}_std_{w}\"] = (df.groupby(\"Patient_ID\")[c]\n",
        "                                     .transform(lambda s: s.rolling(window=w, min_periods=7).std()))\n",
        "        # slope on mean_glu over window w\n",
        "        def slope_rolling(x):\n",
        "            # compute slope using indices 0..len-1 inside window\n",
        "            vals = pd.Series(x)\n",
        "            out = np.full(len(vals), np.nan, dtype=\"float64\")\n",
        "            for i in range(len(vals)):\n",
        "                start = max(0, i - w + 1)\n",
        "                window_vals = vals[start:i+1]\n",
        "                if window_vals.notna().sum() >= 7:\n",
        "                    y = window_vals.dropna().values\n",
        "                    t = np.arange(len(y)).reshape(-1,1)\n",
        "                    lr = LinearRegression().fit(t, y)\n",
        "                    out[i] = lr.coef_[0]\n",
        "            return pd.Series(out, index=vals.index)\n",
        "        df[f\"mean_glu_slope_{w}\"] = df.groupby(\"Patient_ID\")[\"mean_glu\"].transform(slope_rolling)\n",
        "    return df\n",
        "\n",
        "# Compute rolling features on daily CGM (backward-looking, ending at date d)\n",
        "feat_cols = [\"mean_glu\",\"std_glu\",\"tir_70_180\",\"frac_gt_180\",\"frac_lt_70\",\"coverage\",\"n_reads\"]\n",
        "daily_feat = add_rolling_features(daily_cgm, feat_cols, WINDOWS)\n",
        "\n",
        "# Align features to an index date = day + 1\n",
        "daily_feat[\"index_date\"] = daily_feat[\"date\"] + pd.Timedelta(days=1)\n",
        "\n",
        "# Add patient-level demographics (age approx from Birth_year)\n",
        "pi_dem = pi[[\"Patient_ID\",\"Sex\",\"Birth_year\"]].copy()\n",
        "daily_feat = daily_feat.merge(pi_dem, on=\"Patient_ID\", how=\"left\")\n",
        "\n",
        "# Add A1c latest prior to index_date using merge_asof\n",
        "a1c_sorted = a1c.sort_values([\"Patient_ID\",\"date\"])\n",
        "daily_feat = daily_feat.sort_values([\"Patient_ID\",\"index_date\"])\n",
        "daily_feat = pd.merge_asof(\n",
        "    daily_feat, a1c_sorted, left_on=\"index_date\", right_on=\"date\",\n",
        "    by=\"Patient_ID\", direction=\"backward\"\n",
        ").rename(columns={\"a1c\":\"a1c_prior\"}).drop(columns=[\"date_y\"], errors=\"ignore\")\n",
        "\n",
        "# Add diag flags (static)\n",
        "daily_feat = daily_feat.merge(diag_flags, on=\"Patient_ID\", how=\"left\").fillna(0)\n",
        "\n",
        "# --- Build future 90d label proxies ---\n",
        "# 1) Hyperglycemia burden: average frac_gt_180 over next 90 days >= threshold\n",
        "def future_mean_next_n_days(group, col, n=90):\n",
        "    # compute forward rolling mean by reversing\n",
        "    s = group[col].astype(float).copy()\n",
        "    s_rev = s.iloc[::-1].rolling(window=n, min_periods=7).mean().iloc[::-1]\n",
        "    # this mean at day d includes d and next (n-1) days; shift by -1 so it starts next day\n",
        "    fut = s_rev.shift(-1)\n",
        "    return fut\n",
        "\n",
        "daily_feat = daily_feat.sort_values([\"Patient_ID\",\"date_x\"]).rename(columns={\"date_x\":\"date\"})\n",
        "daily_feat[\"future90_frac_gt_180_mean\"] = (daily_feat\n",
        "    .groupby(\"Patient_ID\", group_keys=False)\n",
        "    .apply(lambda g: future_mean_next_n_days(g, \"frac_gt_180\", 90))\n",
        ")\n",
        "\n",
        "# 2) A1c rise in the next 90 days: find next A1c within 90 days via forward merge_asof with tolerance\n",
        "a1c_sorted = a1c_sorted.rename(columns={\"date\":\"a1c_date\",\"a1c\":\"a1c_value\"})\n",
        "# forward asof requires aligning right_on >= left_on; we'll merge on index_date and use direction='forward'\n",
        "tmp = pd.merge_asof(\n",
        "    daily_feat[[\"Patient_ID\",\"index_date\"]].sort_values([\"Patient_ID\",\"index_date\"]),\n",
        "    a1c_sorted.sort_values([\"Patient_ID\",\"a1c_date\"]),\n",
        "    left_on=\"index_date\", right_on=\"a1c_date\", by=\"Patient_ID\", direction=\"forward\",\n",
        "    tolerance=pd.Timedelta(days=90)\n",
        ")\n",
        "daily_feat[\"a1c_future\"] = tmp[\"a1c_value\"].values\n",
        "\n",
        "# Label: 1 if (future90 hyper mean >= threshold) OR (a1c_future - a1c_prior >= A1C_RISE_THRESHOLD)\n",
        "daily_feat[\"label_90d\"] = (\n",
        "    (daily_feat[\"future90_frac_gt_180_mean\"] >= FUTURE_90D_HYPER_MEAN_THRESHOLD) |\n",
        "    ((daily_feat[\"a1c_future\"] - daily_feat[\"a1c_prior\"]) >= A1C_RISE_THRESHOLD)\n",
        ").astype(int)\n",
        "\n",
        "# Keep only rows with at least 30 days of history and 90 days of future available\n",
        "# Approximation: require non-null rolling features for 30d window and non-null future stat\n",
        "mask_hist = daily_feat[\"mean_glu_mean_30\"].notna()\n",
        "mask_future = daily_feat[\"future90_frac_gt_180_mean\"].notna()\n",
        "dataset = daily_feat[mask_hist & mask_future].copy()\n",
        "\n",
        "feature_cols = [c for c in dataset.columns if any(x in c for x in [\"_mean_\",\"_std_\",\"_slope_\",\"coverage\",\"n_reads\"]) and \"future90\" not in c]\n",
        "# Add demographics and diag flags\n",
        "feature_cols += [\"Birth_year\"] + [c for c in dataset.columns if c.startswith(\"diag_\")]\n",
        "# encode Sex simply (M/F/other)\n",
        "if \"Sex\" in dataset.columns:\n",
        "    dataset[\"Sex_enc\"] = dataset[\"Sex\"].astype(\"category\").cat.codes\n",
        "    feature_cols += [\"Sex_enc\"]\n",
        "\n",
        "keep_cols = [\"Patient_ID\",\"date\",\"index_date\",\"label_90d\"] + feature_cols + [\"a1c_prior\",\"a1c_future\"]\n",
        "dataset = dataset[keep_cols].dropna(subset=feature_cols, how=\"all\")\n",
        "\n",
        "print(\"Modeling dataset shape:\", dataset.shape)\n",
        "dataset.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Train, calibrate, evaluate ---\n",
        "# Patient-level split to avoid leakage\n",
        "patients = dataset[\"Patient_ID\"].unique()\n",
        "rng = np.random.default_rng(42)\n",
        "test_pat = set(rng.choice(patients, size=int(0.2*len(patients)), replace=False))\n",
        "\n",
        "train_df = dataset[~dataset[\"Patient_ID\"].isin(test_pat)].copy()\n",
        "test_df  = dataset[ dataset[\"Patient_ID\"].isin(test_pat)].copy()\n",
        "\n",
        "y_tr = train_df[\"label_90d\"].values\n",
        "y_te = test_df[\"label_90d\"].values\n",
        "\n",
        "X_tr = train_df[feature_cols]\n",
        "X_te = test_df[feature_cols]\n",
        "\n",
        "# simple preprocessing (impute medians)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "pre = ColumnTransformer([(\"num\", SimpleImputer(strategy=\"median\"), feature_cols)], remainder=\"drop\")\n",
        "\n",
        "if HAS_XGB:\n",
        "    clf = XGBClassifier(\n",
        "        n_estimators=400, max_depth=4, learning_rate=0.06,\n",
        "        subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
        "        tree_method=\"hist\", random_state=7, eval_metric=\"logloss\",\n",
        "        scale_pos_weight=(y_tr==0).sum()/(y_tr==1).sum() if (y_tr==1).sum()>0 else 1.0\n",
        "    )\n",
        "else:\n",
        "    from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "    clf = HistGradientBoostingClassifier(max_depth=3, learning_rate=0.06, max_iter=400, random_state=7)\n",
        "\n",
        "pipe = Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
        "pipe.fit(X_tr, y_tr)\n",
        "\n",
        "# Calibrate\n",
        "cal = CalibratedClassifierCV(pipe, cv=3, method=\"isotonic\")\n",
        "cal.fit(X_tr, y_tr)\n",
        "\n",
        "# Evaluate\n",
        "proba = cal.predict_proba(X_te)[:,1]\n",
        "auroc = roc_auc_score(y_te, proba) if len(np.unique(y_te))>1 else float(\"nan\")\n",
        "auprc = average_precision_score(y_te, proba) if len(np.unique(y_te))>1 else float(\"nan\")\n",
        "brier = brier_score_loss(y_te, proba)\n",
        "\n",
        "thr = 0.5\n",
        "pred = (proba >= thr).astype(int)\n",
        "cm = confusion_matrix(y_te, pred).tolist()\n",
        "\n",
        "print(\"AUROC:\", auroc, \"AUPRC:\", auprc, \"Brier:\", brier, \"CM@0.5:\", cm)\n",
        "\n",
        "# Save artifacts\n",
        "import joblib\n",
        "joblib.dump(cal, OUT / \"model_calibrated.joblib\")\n",
        "with open(OUT / \"metrics.json\",\"w\") as f:\n",
        "    json.dump({\"AUROC\":auroc, \"AUPRC\":auprc, \"Brier\":brier, \"ConfusionMatrix@0.5\":cm}, f, indent=2)\n",
        "with open(OUT / \"feature_meta.json\",\"w\") as f:\n",
        "    json.dump({\"feature_cols\": feature_cols}, f, indent=2)\n",
        "\n",
        "# Save dataset for app/demo\n",
        "dataset.to_parquet(OUT / \"feature_dataset.parquet\", index=False)\n",
        "\n",
        "# Plots (matplotlib only)\n",
        "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
        "fig, ax = plt.subplots()\n",
        "RocCurveDisplay.from_predictions(y_te, proba, ax=ax); plt.savefig(PLOTS / \"roc_curve.png\"); plt.close()\n",
        "fig, ax = plt.subplots()\n",
        "PrecisionRecallDisplay.from_predictions(y_te, proba, ax=ax); plt.savefig(PLOTS / \"pr_curve.png\"); plt.close()\n",
        "\n",
        "# Calibration plot\n",
        "from sklearn.calibration import calibration_curve\n",
        "prob_true, prob_pred = calibration_curve(y_te, proba, n_bins=10, strategy='quantile')\n",
        "plt.plot(prob_pred, prob_true, marker='o'); plt.plot([0,1],[0,1],'--')\n",
        "plt.xlabel(\"Predicted\"); plt.ylabel(\"Observed\"); plt.title(\"Calibration\")\n",
        "plt.savefig(PLOTS / \"calibration.png\"); plt.close()\n",
        "\n",
        "print(\"Artifacts saved in:\", OUT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Explainability (SHAP) ---\n",
        "if HAS_SHAP:\n",
        "    # SHAP with predict_proba callable works through calibrated pipeline\n",
        "    # For speed, sample\n",
        "    sample = train_df.sample(n=min(200, len(train_df)), random_state=7)\n",
        "    Xs = sample[feature_cols]\n",
        "    explainer = shap.Explainer(cal.predict_proba, Xs, feature_names=feature_cols)\n",
        "    sv = explainer(Xs)\n",
        "    shap.plots.beeswarm(sv, max_display=20, show=False)\n",
        "    plt.tight_layout(); plt.savefig(PLOTS / \"shap_global.png\"); plt.close()\n",
        "    print(\"Saved SHAP beeswarm to\", PLOTS / \"shap_global.png\")\n",
        "else:\n",
        "    print(\"SHAP not installed; skip explainability here. (Install shap to enable.)\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
